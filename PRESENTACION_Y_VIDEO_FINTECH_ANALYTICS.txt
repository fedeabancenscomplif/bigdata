================================================================================
                    🚀 FINTECH ANALYTICS - ANÁLISIS DE ONBOARDING Y A/B TESTING
                    ================================================================
                    PROYECTO DE BIG DATA ENGINEERING
                    ================================================================

================================================================================
                                    PRESENTACIÓN
================================================================================

📋 AGENDA
---------
1. Contexto del Negocio
2. Arquitectura de la Solución
3. Tecnologías Utilizadas
4. Proceso de Desarrollo
5. Pre-procesamiento y Transformación
6. Decisiones Técnicas
7. Resultados y Métricas
8. Conclusiones y Próximos Pasos

================================================================================
1. CONTEXTO DEL NEGOCIO
================================================================================

🏦 PROBLEMA IDENTIFICADO
• Pérdida de usuarios durante el proceso de onboarding
• Falta de visibilidad en el comportamiento de usuarios
• Necesidad de optimización de la experiencia de usuario
• Implementación de A/B testing para validar mejoras

🎯 OBJETIVOS DEL PROYECTO
• Reducir la tasa de abandono en onboarding
• Analizar el funnel de activación de usuarios
• Implementar A/B testing para validar mejoras
• Crear dashboard en tiempo real para monitoreo
• Optimizar la experiencia del usuario

📊 MÉTRICAS CLAVE (KPIs)
• Drop Rate: Usuarios que abandonan el proceso
• Activation Rate: Usuarios que completan activación
• Setup Rate: Usuarios que configuran su cuenta
• Habit Rate: Usuarios que desarrollan hábito de uso

================================================================================
2. ARQUITECTURA DE LA SOLUCIÓN
================================================================================

┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Datasets CSV  │    │   Apache Spark  │    │  Apache Cassandra│
│                 │    │   (PySpark)     │    │                 │
│ • lk_onboarding │───▶│   ETL Pipeline  │───▶│   Base de Datos │
│ • dim_users     │    │                 │    │                 │
│ • transactions  │    │ • Limpieza      │    │ • Métricas      │
│                 │    │ • Transformación│    │ • A/B Testing   │
└─────────────────┘    │ • Carga         │    │ • Tiempo Real   │
                       └─────────────────┘    └─────────────────┘
                                                       │
                                                       ▼
                                              ┌─────────────────┐
                                              │   Streamlit     │
                                              │   Dashboard     │
                                              │                 │
                                              │ • Visualización │
                                              │ • Análisis      │
                                              │ • Tiempo Real   │
                                              └─────────────────┘

FLUJO DE DATOS:
1. Ingesta: Datasets CSV → Apache Spark
2. Procesamiento: ETL con PySpark → Transformación y cálculo de métricas
3. Almacenamiento: Resultados → Apache Cassandra
4. Visualización: Dashboard Streamlit → Consume datos directamente desde Cassandra

================================================================================
3. TECNOLOGÍAS UTILIZADAS
================================================================================

🛠️ PROCESAMIENTO DE DATOS
• Apache Spark 3.5.0: Procesamiento distribuido y ETL
• PySpark: API de Python para Spark
• Pandas 2.1.4: Manipulación de datos

🗄️ BASE DE DATOS
• Apache Cassandra: Base de datos NoSQL para tiempo real
• Cassandra Driver 3.28.0: Conector Python

📊 DASHBOARD Y VISUALIZACIÓN
• Streamlit 1.28.1: Framework para dashboards web
• Plotly 5.17.0: Visualizaciones interactivas
• Matplotlib 3.8.2: Visualizaciones básicas
• Seaborn 0.13.0: Visualizaciones estadísticas

🏗️ INFRAESTRUCTURA
• Docker: Containerización de servicios
• Docker Compose: Orquestación de contenedores

================================================================================
4. PROCESO DE DESARROLLO
================================================================================

🔧 FASE 1: ANÁLISIS EXPLORATORIO
✅ Exploración de datasets: Identificación de estructura y calidad
✅ Detección de inconsistencias: Valores faltantes y duplicados
✅ Validación de datos: Verificación de integridad
✅ Documentación de hallazgos: Reporte de calidad de datos

🔧 FASE 2: DISEÑO DE ARQUITECTURA
✅ Selección de tecnologías: Spark + Cassandra + Streamlit
✅ Diseño del pipeline: ETL optimizado
✅ Definición de métricas: KPIs de negocio
✅ Planificación de A/B testing: Distribución 5%/95%

🔧 FASE 3: DESARROLLO DEL ETL
✅ Implementación del pipeline: Código PySpark
✅ Limpieza de datos: Filtrado y validación
✅ Cálculo de métricas: Lógicas de negocio
✅ Integración con Cassandra: Almacenamiento optimizado

🔧 FASE 4: DESARROLLO DEL DASHBOARD
✅ Interfaz de usuario: Streamlit responsive
✅ Visualizaciones: Gráficos interactivos
✅ Conexión en tiempo real: Cassandra directa
✅ Funcionalidades avanzadas: Filtros y exportación

================================================================================
5. PRE-PROCESAMIENTO Y TRANSFORMACIÓN
================================================================================

🧹 LIMPIEZA DE DATOS
• Filtrado por segmento: Solo usuarios con segmento válido (1 o 2)
• Resolución de inconsistencias: Segmentos duplicados por usuario
• Formateo de fechas: Conversión a formato estándar
• Validación de integridad: Verificación de relaciones entre tablas

🔄 TRANSFORMACIÓN DE DATOS
• Cálculo de Drop Rate: drop = 1 si return = 0
• Cálculo de Activation Rate: Basado en existencia de activacion_dt
• Cálculo de Setup Rate: Basado en existencia de setup_dt
• Cálculo de Habit Rate: Diferenciado por segmento
  - Individuals: ≥5 días distintos de transacciones en 30 días
  - Sellers: ≥5 cobros (tipos 8 o 9) en 30 días

🔬 A/B TESTING
• Asignación aleatoria: Distribución 5% control, 95% tratamiento
• Simulación de experimento: Para validar mejoras futuras
• Métricas comparativas: Análisis de diferencias entre grupos

================================================================================
6. DECISIONES TÉCNICAS
================================================================================

⚡ SELECCIÓN DE SPARK
• Procesamiento distribuido: Manejo de grandes volúmenes
• Optimización de memoria: Cache inteligente
• Escalabilidad: Crecimiento futuro del proyecto
• Integración con Cassandra: Compatibilidad nativa

🗄️ SELECCIÓN DE CASSANDRA
• Consultas rápidas: Optimizado para lecturas
• Escalabilidad horizontal: Manejo de grandes volúmenes
• Alta disponibilidad: Tolerancia a fallos
• Tiempo real: Datos actualizados instantáneamente

📊 SELECCIÓN DE STREAMLIT
• Desarrollo rápido: Prototipado ágil
• Interfaz intuitiva: Fácil de usar
• Integración Python: Compatibilidad con el stack
• Deployment simple: Fácil despliegue

🏗️ ARQUITECTURA DE DATOS
• LEFT JOINs: Mantener todos los usuarios de onboarding
• Filtrado inteligente: Solo datos relevantes
• Cache optimizado: 5 minutos para mejor rendimiento
• Manejo de errores: Robustez en la conexión

================================================================================
7. RESULTADOS Y MÉTRICAS
================================================================================

📊 FUNNEL DE ONBOARDING
Usuarios Registrados (100%)
         ↓
    Activación (~60-80%)
         ↓
      Setup (~40-60%)
         ↓
     Hábito (~20-40%)

👥 ANÁLISIS POR SEGMENTO
• Individuals: Usuarios individuales
• Sellers: Vendedores/comerciantes
• Diferencias significativas en tasas de conversión

🔬 A/B TESTING
• Grupo Control: 5% de usuarios (baseline)
• Grupo Treatment: 95% de usuarios (nueva experiencia)
• Métricas comparativas: Diferencias estadísticas

📈 DASHBOARD EN TIEMPO REAL
• Configuración dinámica: Host, puerto, keyspace, tabla
• Prueba de conexión: Verificación automática
• Navegación modular: 5 secciones principales
• Exportación de datos: Descarga de datos filtrados

================================================================================
8. CONCLUSIONES Y PRÓXIMOS PASOS
================================================================================

🎯 LOGROS ALCANZADOS
✅ Pipeline ETL completo: Procesamiento automatizado
✅ Dashboard en tiempo real: Monitoreo continuo
✅ A/B testing implementado: Validación de mejoras
✅ Métricas de negocio: KPIs claros y medibles
✅ Arquitectura escalable: Preparada para crecimiento

💰 BENEFICIOS DEL NEGOCIO
• Reducción de drop rate: 10-20% esperado
• Incremento en activation rate: 15-25% esperado
• Mejora en habit formation: 20-30% esperado
• Visibilidad completa: Dashboard en tiempo real

🚀 PRÓXIMOS PASOS
1. Implementación en producción: Despliegue del pipeline
2. Monitoreo continuo: Seguimiento de métricas
3. Optimización basada en datos: Mejoras iterativas
4. Expansión a otros países: Escalabilidad geográfica
5. Machine Learning: Personalización de experiencia

📚 LECCIONES APRENDIDAS
• Importancia de la calidad de datos: Filtrado crítico
• Valor del tiempo real: Cassandra para dashboards
• Eficiencia de Spark: Procesamiento distribuido
• Simplicidad de Streamlit: Desarrollo ágil

================================================================================
                                    VIDEO EXPLICATIVO
================================================================================

🎥 GUION PARA VIDEO EXPLICATIVO (15-16 minutos)

================================================================================
INTRODUCCIÓN (0:00 - 1:00)
================================================================================

"Bienvenidos a la presentación del proyecto Fintech Analytics. 
Hoy les mostraré cómo desarrollamos un sistema completo de análisis 
de onboarding y A/B testing para una plataforma fintech.

El objetivo era reducir la pérdida de usuarios durante el proceso 
de onboarding y crear un dashboard en tiempo real para monitorear 
las métricas clave del negocio.

Este proyecto demuestra las capacidades de la ingeniería de datos 
para resolver problemas reales de negocio utilizando tecnologías 
de Big Data modernas."

================================================================================
CONTEXTO DEL NEGOCIO (1:00 - 2:30)
================================================================================

"El problema que identificamos fue la pérdida significativa de 
usuarios durante el proceso de onboarding. La empresa necesitaba:

1. Visibilidad en tiempo real del comportamiento de usuarios
2. Análisis del funnel de activación
3. Implementación de A/B testing para validar mejoras
4. Optimización de la experiencia del usuario

Para esto definimos 4 métricas clave que nos permitirían medir 
el éxito del proceso de onboarding:

- Drop Rate: Usuarios que abandonan el proceso
- Activation Rate: Usuarios que completan la activación
- Setup Rate: Usuarios que configuran su cuenta
- Habit Rate: Usuarios que desarrollan hábito de uso

Estas métricas nos darían una visión completa del funnel de 
onboarding y nos permitirían identificar puntos de fricción."

================================================================================
ARQUITECTURA DE LA SOLUCIÓN (2:30 - 4:00)
================================================================================

"La arquitectura que diseñamos utiliza tecnologías de Big Data 
modernas. Les muestro el flujo completo:

Los datos comienzan en archivos CSV con información de onboarding, 
usuarios y transacciones. Estos datos se procesan con Apache Spark 
usando PySpark, donde realizamos la limpieza, transformación y 
cálculo de métricas.

Los resultados se almacenan en Apache Cassandra, una base de datos 
NoSQL optimizada para consultas rápidas y tiempo real.

Finalmente, un dashboard desarrollado con Streamlit consume los 
datos directamente desde Cassandra, proporcionando visualizaciones 
interactivas y análisis en tiempo real.

Esta arquitectura nos permite:
- Procesar grandes volúmenes de datos eficientemente
- Almacenar datos optimizados para consultas rápidas
- Visualizar resultados en tiempo real
- Escalar según las necesidades del negocio"

================================================================================
TECNOLOGÍAS UTILIZADAS (4:00 - 5:30)
================================================================================

"Para el procesamiento de datos elegimos Apache Spark 3.5.0 por 
su capacidad de procesamiento distribuido y escalabilidad. 
PySpark nos permite desarrollar en Python, facilitando el 
desarrollo y mantenimiento.

Para la base de datos seleccionamos Apache Cassandra por su 
optimización para lecturas rápidas y alta disponibilidad. 
El Cassandra Driver nos permite conectar desde Python.

Para el dashboard utilizamos Streamlit, que nos permite crear 
interfaces web interactivas rápidamente. Plotly proporciona 
visualizaciones avanzadas y responsivas.

Todo se ejecuta en contenedores Docker para facilitar el 
despliegue y mantenimiento.

Esta combinación de tecnologías nos da:
- Procesamiento distribuido eficiente
- Almacenamiento optimizado para tiempo real
- Desarrollo rápido de interfaces
- Despliegue y mantenimiento simplificado"

================================================================================
PROCESO DE DESARROLLO (5:30 - 7:00)
================================================================================

"El desarrollo se dividió en 4 fases principales:

En la Fase 1 realizamos análisis exploratorio de los datos, 
identificando inconsistencias y validando la calidad de los 
datos. Documentamos todos los hallazgos para guiar el desarrollo.

En la Fase 2 diseñamos la arquitectura completa, seleccionando 
las tecnologías y definiendo las métricas de negocio. 
Planificamos el A/B testing con distribución 5% control, 
95% tratamiento.

En la Fase 3 desarrollamos el pipeline ETL con PySpark, 
implementando la limpieza de datos, cálculo de métricas y 
integración con Cassandra.

En la Fase 4 creamos el dashboard con Streamlit, conectándolo 
directamente a Cassandra para datos en tiempo real.

Cada fase fue iterativa, permitiéndonos validar y mejorar 
continuamente la solución."

================================================================================
PRE-PROCESAMIENTO Y TRANSFORMACIÓN (7:00 - 8:30)
================================================================================

"En el pre-procesamiento implementamos filtros importantes:

Filtramos usuarios sin segmento válido, manteniendo solo 
segmento 1 (Individuals) y 2 (Sellers). Esto mejora la 
calidad del análisis.

Resolvimos inconsistencias de segmentos duplicados, 
tomando el segmento más frecuente por usuario.

Calculamos las métricas de negocio:
- Drop Rate: usuarios que no vuelven después del primer login
- Activation Rate: usuarios que completan la activación
- Setup Rate: usuarios que configuran su cuenta
- Habit Rate: diferenciado por segmento

Para Individuals: 5 días distintos de transacciones en 30 días
Para Sellers: 5 cobros (tipos 8 o 9) en 30 días

Implementamos A/B testing con asignación aleatoria para 
simular experimentos futuros.

Esta transformación nos permite tener datos limpios y 
métricas consistentes para el análisis."

================================================================================
DECISIONES TÉCNICAS (8:30 - 10:00)
================================================================================

"Las decisiones técnicas fueron fundamentales para el éxito:

Elegimos Spark por su capacidad de procesamiento distribuido 
y optimización de memoria. La integración nativa con Cassandra 
facilita el flujo de datos.

Cassandra fue seleccionada por su optimización para lecturas 
rápidas y escalabilidad horizontal. Su alta disponibilidad 
garantiza que el dashboard siempre tenga datos disponibles.

Streamlit nos permitió desarrollar el dashboard rápidamente 
con una interfaz intuitiva. La integración con Python facilita 
el mantenimiento.

En la arquitectura de datos usamos LEFT JOINs para mantener 
todos los usuarios de onboarding, implementamos cache inteligente 
de 5 minutos y robusto manejo de errores.

Estas decisiones nos permiten:
- Procesar datos eficientemente
- Almacenar para consultas rápidas
- Desarrollar interfaces rápidamente
- Mantener alta disponibilidad"

================================================================================
DEMOSTRACIÓN DEL PIPELINE (10:00 - 12:00)
================================================================================

"Ahora les muestro el funcionamiento del pipeline:

[DEMOSTRACIÓN EN VIVO]

1. Ejecutamos el ETL: python3 etl_pipeline_clean.py
   - Carga de datasets CSV
   - Filtrado de usuarios sin segmento
   - Cálculo de métricas
   - Almacenamiento en Cassandra

2. Ejecutamos el dashboard: streamlit run dashboard_cassandra.py
   - Conexión a Cassandra
   - Carga de datos en tiempo real
   - Visualización del funnel
   - Análisis por segmento
   - Comparación A/B testing

Como pueden ver, el dashboard muestra:
- Funnel de onboarding completo
- Métricas por segmento (Individuals vs Sellers)
- Análisis A/B testing con diferencias
- Datos raw filtrables y exportables

El sistema es completamente funcional y está listo para 
producción."

================================================================================
RESULTADOS Y MÉTRICAS (12:00 - 13:30)
================================================================================

"Los resultados obtenidos muestran un funnel de onboarding 
claro y medible:

En el registro tenemos 100% de usuarios que inician el proceso.
En activación vemos tasas del 60-80%, dependiendo del segmento.
En setup las tasas bajan a 40-60%.
En hábito alcanzamos 20-40% de retención.

El análisis por segmento revela diferencias significativas:
- Individuals tienen mejor activación
- Sellers muestran mejor hábito de uso

El A/B testing está configurado para validar mejoras futuras, 
con distribución 5% control, 95% tratamiento.

El dashboard proporciona visibilidad completa en tiempo real, 
con configuración dinámica y exportación de datos.

Estas métricas nos permiten:
- Identificar puntos de fricción
- Comparar segmentos de usuarios
- Validar mejoras con A/B testing
- Monitorear en tiempo real"

================================================================================
CONCLUSIONES (13:30 - 15:00)
================================================================================

"En conclusión, hemos logrado desarrollar un sistema completo 
de análisis de onboarding que incluye:

✅ Pipeline ETL automatizado con Spark
✅ Dashboard en tiempo real con Cassandra
✅ A/B testing implementado
✅ Métricas de negocio claras
✅ Arquitectura escalable

Los beneficios esperados para el negocio incluyen:
- Reducción del drop rate en 10-20%
- Incremento en activation rate de 15-25%
- Mejora en habit formation de 20-30%
- Visibilidad completa del proceso

Los próximos pasos incluyen:
1. Implementación en producción
2. Monitoreo continuo de métricas
3. Optimización basada en datos
4. Expansión a otros países
5. Implementación de Machine Learning

Este proyecto demuestra las capacidades de la ingeniería de 
datos para resolver problemas reales de negocio utilizando 
tecnologías de Big Data modernas.

El sistema está listo para producción y puede escalar según 
las necesidades de la empresa."

================================================================================
CIERRE (15:00 - 15:30)
================================================================================

"Gracias por su atención. Este proyecto representa un ejemplo 
concreto de cómo las tecnologías de Big Data pueden transformar 
la experiencia del usuario y optimizar procesos de negocio.

El sistema está listo para producción y puede escalar según 
las necesidades de la empresa.

¿Tienen alguna pregunta sobre la implementación o los resultados?"

================================================================================
CHECKLIST PARA EL VIDEO
================================================================================

PREPARACIÓN:
□ Configurar entorno de desarrollo
□ Tener datos de ejemplo cargados
□ Preparar Cassandra ejecutándose
□ Tener el dashboard funcionando

GRABACIÓN:
□ Introducción clara del proyecto
□ Explicación de la arquitectura
□ Demostración del ETL en vivo
□ Demostración del dashboard en vivo
□ Mostrar resultados y métricas
□ Conclusiones y próximos pasos

POST-PRODUCCIÓN:
□ Editar para mantener 15-16 minutos
□ Agregar transiciones entre secciones
□ Incluir capturas de pantalla del código
□ Agregar subtítulos si es necesario
□ Verificar calidad de audio y video

================================================================================
COMANDOS PARA EJECUTAR EL PROYECTO
================================================================================

1. INSTALAR DEPENDENCIAS:
   pip install -r requirements.txt

2. LEVANTAR CASSANDRA:
   docker-compose up -d

3. EJECUTAR ETL:
   python3 etl_pipeline_clean.py

4. EJECUTAR DASHBOARD:
   streamlit run dashboard_cassandra.py

================================================================================
ESTRUCTURA DEL PROYECTO
================================================================================

tpfinal_bigdata/
├── artifacts/                          # Resultados del ETL
│   └── user_onboarding_metrics_clean/  # Métricas limpias
├── etl_pipeline_clean.py              # ETL principal
├── dashboard_cassandra.py              # Dashboard
├── requirements.txt                    # Dependencias
├── docker-compose.yml                 # Configuración
├── README.md                          # Documentación
├── lk_onboarding.csv                  # Dataset onboarding
├── dim_users.csv                      # Dataset usuarios
└── bt_users_transactions.csv          # Dataset transacciones

================================================================================
FIN DEL DOCUMENTO
================================================================================

Proyecto desarrollado para el curso de Big Data Engineering
Análisis de onboarding y optimización de experiencia de usuario
Tecnologías: Spark + Cassandra + Streamlit 